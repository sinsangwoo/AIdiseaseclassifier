"""
ëª¨ë¸ ê´€ë¦¬ ì„œë¹„ìŠ¤ (Phase 3-4)

ONNX ëª¨ë¸ì˜ ë¡œë”©, ìºì‹±, ì˜ˆì¸¡ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ ë ˆì´ì–´ì…ë‹ˆë‹¤.
ê¸°ì¡´ ModelPredictorì˜ ë¡œì§ì„ í™•ì¥í•˜ì—¬ ê´€ì‹¬ì‚¬ ë¶„ë¦¬ì™€ ìºì‹±ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import time
import hashlib
from functools import lru_cache
from typing import Dict, List, Optional

import numpy as np

from backend.models import ModelPredictor
from backend.utils import get_logger, ModelLoadError, PredictionError


class ModelService:
    """
    ëª¨ë¸ ê´€ë¦¬ ë° ì˜ˆì¸¡ ì„œë¹„ìŠ¤ (Phase 3)
    
    ì—­í• :
    - ëª¨ë¸ ë¡œë”© ë° ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬
    - ì˜ˆì¸¡ ê²°ê³¼ ìºì‹± (LRU Cache)
    - ëª¨ë¸ ì›Œë°ì—… ë° ì„±ëŠ¥ ìµœì í™”
    - í†µê³„ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    """
    
    def __init__(
        self,
        model_path: str,
        labels_path: str,
        enable_cache: bool = True,
        cache_size: int = 128
    ):
        """
        Args:
            model_path: ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            labels_path: ë ˆì´ë¸” íŒŒì¼ ê²½ë¡œ
            enable_cache: ì˜ˆì¸¡ ìºì‹± í™œì„±í™” ì—¬ë¶€
            cache_size: LRU ìºì‹œ ìµœëŒ€ í¬ê¸°
        """
        self.model_path = model_path
        self.labels_path = labels_path
        self.enable_cache = enable_cache
        self.cache_size = cache_size
        
        self.logger = get_logger('aiclassifier.model_service')
        
        # ë‚´ë¶€ ModelPredictor ì¸ìŠ¤í„´ìŠ¤
        self._predictor: Optional[ModelPredictor] = None
        
        # í†µê³„
        self.stats = {
            'total_predictions': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'total_inference_time_ms': 0.0,
            'warmup_completed': False
        }
        
        # ìºì‹œ í¬ê¸° ì„¤ì • (lru_cacheëŠ” í•¨ìˆ˜ ë°ì½”ë ˆì´í„°ì´ë¯€ë¡œ ë™ì  ì„¤ì • ë¶ˆê°€)
        # ì‹¤ì œ ìºì‹±ì€ _cached_predict ë©”ì„œë“œì—ì„œ ìˆ˜í–‰
        self.logger.info(f"âœ“ ModelService ì´ˆê¸°í™” (ìºì‹±: {enable_cache}, ìºì‹œ í¬ê¸°: {cache_size})")
    
    def load_model(self) -> None:
        """ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”"""
        if self._predictor is None:
            self._predictor = ModelPredictor(
                model_path=self.model_path,
                labels_path=self.labels_path
            )
            self._predictor.load_model()
            
            self.logger.info("âœ“ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
            # ì›Œë°ì—… ìˆ˜í–‰
            if self.stats['warmup_completed'] is False:
                self._warmup_model()
    
    def _warmup_model(self) -> None:
        """
        ëª¨ë¸ ì›Œë°ì—…
        
        ì²« ì˜ˆì¸¡ì€ ëª¨ë¸ ì´ˆê¸°í™”ë¡œ ì¸í•´ ëŠë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
        ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ì „ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
        """
        try:
            self.logger.info("ğŸ”¥ ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
            
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (1, 3, 224, 224) - NCHW í¬ë§·
            dummy_input = np.random.rand(1, 3, 224, 224).astype(np.float32)
            
            start_time = time.time()
            _ = self._predictor.predict(dummy_input)
            warmup_time = (time.time() - start_time) * 1000
            
            self.stats['warmup_completed'] = True
            self.logger.info(f"âœ“ ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ ({warmup_time:.0f}ms)")
            
        except Exception as e:
            self.logger.warning(f"ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")
    
    def is_ready(self) -> bool:
        """ëª¨ë¸ ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
        return self._predictor is not None and self._predictor.is_ready()
    
    def predict(
        self,
        processed_image: np.ndarray,
        use_cache: Optional[bool] = None
    ) -> List[Dict[str, any]]:
        """
        ì´ë¯¸ì§€ ì˜ˆì¸¡ (ìºì‹± ì§€ì›)
        
        Args:
            processed_image: ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ (numpy array)
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ ê¸°ë³¸ ì„¤ì • ë”°ë¦„)
        
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.is_ready():
            raise PredictionError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        self.stats['total_predictions'] += 1
        
        # ìºì‹± ì„¤ì •
        should_use_cache = use_cache if use_cache is not None else self.enable_cache
        
        # ìºì‹œ ì‚¬ìš© ì‹œ ì´ë¯¸ì§€ í•´ì‹œ ê³„ì‚°
        if should_use_cache:
            image_hash = self._compute_image_hash(processed_image)
            
            # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
            try:
                cached_result = self._cached_predict(image_hash)
                if cached_result is not None:
                    self.stats['cache_hits'] += 1
                    self.logger.debug(f"âœ“ ìºì‹œ íˆíŠ¸ (í•´ì‹œ: {image_hash[:8]}...)")
                    return cached_result
            except:
                pass  # ìºì‹œ ë¯¸ìŠ¤
            
            self.stats['cache_misses'] += 1
        
        # ì‹¤ì œ ì˜ˆì¸¡ ìˆ˜í–‰
        start_time = time.time()
        predictions = self._predictor.predict(processed_image)
        inference_time = (time.time() - start_time) * 1000
        
        self.stats['total_inference_time_ms'] += inference_time
        
        # ìºì‹œì— ì €ì¥
        if should_use_cache:
            self._save_to_cache(image_hash, predictions)
        
        return predictions
    
    def _compute_image_hash(self, image_array: np.ndarray) -> str:
        """
        ì´ë¯¸ì§€ ë°°ì—´ì˜ í•´ì‹œê°’ ê³„ì‚°
        
        Args:
            image_array: numpy ì´ë¯¸ì§€ ë°°ì—´
        
        Returns:
            SHA-256 í•´ì‹œ ë¬¸ìì—´
        """
        # numpy ë°°ì—´ì„ bytesë¡œ ë³€í™˜
        image_bytes = image_array.tobytes()
        return hashlib.sha256(image_bytes).hexdigest()
    
    @lru_cache(maxsize=128)
    def _cached_predict(self, image_hash: str) -> Optional[List[Dict]]:
        """
        ìºì‹œëœ ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ (LRU Cache)
        
        Note: lru_cacheëŠ” ë™ì¼í•œ ì¸ìì— ëŒ€í•´ ê²°ê³¼ë¥¼ ìºì‹±
        ì´ ë©”ì„œë“œëŠ” í•­ìƒ Noneì„ ë°˜í™˜í•˜ê³ , ì‹¤ì œ ìºì‹±ì€ _save_to_cacheì—ì„œ ìˆ˜í–‰
        """
        return None
    
    def _save_to_cache(self, image_hash: str, predictions: List[Dict]) -> None:
        """
        ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        
        functools.lru_cacheëŠ” í•¨ìˆ˜ì˜ ë°˜í™˜ê°’ì„ ìºì‹±í•˜ë¯€ë¡œ,
        ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ dict ê¸°ë°˜ ìºì‹œ ì‚¬ìš©
        """
        # ê°„ë‹¨í•œ dict ìºì‹œ (ì‹¤ì œ êµ¬í˜„)
        if not hasattr(self, '_cache'):
            self._cache = {}
        
        self._cache[image_hash] = predictions
        
        # LRU ì •ì±…: ìºì‹œ í¬ê¸° ì´ˆê³¼ ì‹œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
        if len(self._cache) > self.cache_size:
            oldest_key = next(iter(self._cache))
            del self._cache[oldest_key]
    
    def _get_from_cache(self, image_hash: str) -> Optional[List[Dict]]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        if not hasattr(self, '_cache'):
            return None
        return self._cache.get(image_hash)
    
    def get_model_info(self) -> Dict[str, any]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        if self._predictor:
            return self._predictor.get_model_info()
        else:
            return {
                'status': 'not_loaded',
                'model_path': self.model_path,
                'labels_path': self.labels_path
            }
    
    def get_statistics(self) -> Dict[str, any]:
        """
        ì„œë¹„ìŠ¤ í†µê³„ ì¡°íšŒ
        
        Returns:
            í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        cache_hit_rate = 0.0
        if self.stats['total_predictions'] > 0:
            cache_hit_rate = (
                self.stats['cache_hits'] / self.stats['total_predictions']
            ) * 100
        
        avg_inference_time = 0.0
        if self.stats['cache_misses'] > 0:
            avg_inference_time = (
                self.stats['total_inference_time_ms'] / self.stats['cache_misses']
            )
        
        return {
            'total_predictions': self.stats['total_predictions'],
            'cache_enabled': self.enable_cache,
            'cache_size': self.cache_size,
            'cache_hits': self.stats['cache_hits'],
            'cache_misses': self.stats['cache_misses'],
            'cache_hit_rate_percent': round(cache_hit_rate, 2),
            'avg_inference_time_ms': round(avg_inference_time, 2),
            'total_inference_time_ms': round(self.stats['total_inference_time_ms'], 2),
            'warmup_completed': self.stats['warmup_completed']
        }
    
    def clear_cache(self) -> None:
        """ìºì‹œ ì´ˆê¸°í™”"""
        if hasattr(self, '_cache'):
            self._cache.clear()
        self._cached_predict.cache_clear()
        self.stats['cache_hits'] = 0
        self.stats['cache_misses'] = 0
        self.logger.info("âœ“ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_cache_info(self) -> Dict[str, int]:
        """
        ìºì‹œ ì •ë³´ ì¡°íšŒ
        
        Returns:
            ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤/í¬ê¸° ì •ë³´
        """
        cache_info = self._cached_predict.cache_info()
        current_size = len(self._cache) if hasattr(self, '_cache') else 0
        
        return {
            'hits': self.stats['cache_hits'],
            'misses': self.stats['cache_misses'],
            'maxsize': self.cache_size,
            'currsize': current_size
        }
