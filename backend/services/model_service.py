"""
ëª¨ë¸ ê´€ë¦¬ ì„œë¹„ìŠ¤ (Phase 3 â€” ìºì‹œ ìˆ˜ìˆ  ì™„ë£Œ)

PyTorch ëª¨ë¸ì˜ ë¡œë”©, ìºì‹±, ì˜ˆì¸¡ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ ë ˆì´ì–´ì…ë‹ˆë‹¤.

ìºì‹œ ì•„í‚¤í…ì²˜ (ìˆ˜ìˆ  ì „í›„)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ìˆ˜ìˆ  ì „:
    predict() â†’ _cached_predict(hash)  â† lru_cache ë°ì½”ë ˆì´í„°
                  â””â”€ í•­ìƒ None ë°˜í™˜ (ì„¤ê³„ ì˜¤ë¥˜)
                _save_to_cache(hash, result) â†’ self._cache[hash] = result
                  â””â”€ ì €ì¥ì€ ë˜ì§€ë§Œ ì¡°íšŒ ê²½ë¡œì— ì—°ê²°ë˜ì§€ ì•ŠìŒ
                ê²°ê³¼: cache_hit ë°œìƒ ë¶ˆê°€ (100% miss)

  ìˆ˜ìˆ  í›„:
    predict() â†’ _get_from_cache(hash)  â† self._cache dict ì§ì ‘ ì¡°íšŒ
                  â””â”€ hitì´ë©´ ì¦‰ì‹œ ë°˜í™˜
                _save_to_cache(hash, result) â†’ self._cache[hash] = result
                  â””â”€ OrderedDict ê¸°ë°˜ LRU ì •ì±… ì ìš©
    ë°˜í™˜ê°’: (predictions, from_cache: bool)  â† app.pyì—ì„œ ì‚¬ìš© ê°€ëŠ¥
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""

import time
import hashlib
from collections import OrderedDict
from typing import Dict, List, Optional, Tuple

import numpy as np
import numpy as np

from backend.models import ModelPredictor
from backend.utils import get_logger, ModelLoadError, PredictionError


class ModelService:
    """
    ëª¨ë¸ ê´€ë¦¬ ë° ì˜ˆì¸¡ ì„œë¹„ìŠ¤

    ì—­í• :
    - ëª¨ë¸ ë¡œë”© ë° ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬
    - ì˜ˆì¸¡ ê²°ê³¼ ìºì‹± (OrderedDict ê¸°ë°˜ LRU)
    - ëª¨ë¸ ì›Œë°ì—… ë° ì„±ëŠ¥ ìµœì í™”
    - í†µê³„ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    """

    def __init__(
        self,
        model_path: str,
        labels_path: str,
        enable_cache: bool = True,
        cache_size: int = 128
    ):
        """
        Args:
            model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.pt ì§€ì›, ì—†ìœ¼ë©´ pretrained ì‚¬ìš©)
            labels_path: ë ˆì´ë¸” íŒŒì¼ ê²½ë¡œ
            enable_cache: ì˜ˆì¸¡ ìºì‹± í™œì„±í™” ì—¬ë¶€
            cache_size: LRU ìºì‹œ ìµœëŒ€ í¬ê¸°
        """
        self.model_path = model_path
        self.labels_path = labels_path
        self.enable_cache = enable_cache
        self.cache_size = cache_size

        self.logger = get_logger('aiclassifier.model_service')

        # ë‚´ë¶€ ModelPredictor ì¸ìŠ¤í„´ìŠ¤
        self._predictor: Optional[ModelPredictor] = None

        # OrderedDict ê¸°ë°˜ LRU ìºì‹œ
        # - ì¡°íšŒÂ·ì €ì¥ ì‹œ í•´ë‹¹ í‚¤ë¥¼ endë¡œ ì´ë™
        # - í¬ê¸° ì´ˆê³¼ ì‹œ ê°€ì¥ ì•(ì˜¤ë˜ëœ í•­ëª©)ì„ ì œê±°
        self._cache: OrderedDict = OrderedDict()

        # í†µê³„
        self.stats = {
            'total_predictions': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'total_inference_time_ms': 0.0,
            'warmup_completed': False
        }

        self.logger.info(
            f"âœ“ ModelService ì´ˆê¸°í™” (ìºì‹±: {enable_cache}, ìºì‹œ í¬ê¸°: {cache_size})"
        )

    # â”€â”€â”€ ëª¨ë¸ ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def load_model(self) -> None:
        """ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”"""
        if self._predictor is None:
            self._predictor = ModelPredictor(
                model_path=self.model_path,
                labels_path=self.labels_path
            )
            self._predictor.load_model()
            self.logger.info("âœ“ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

            # ì›Œë°ì—… ìˆ˜í–‰
            if not self.stats['warmup_completed']:
                self._warmup_model()

    def _warmup_model(self) -> None:
        """
        ëª¨ë¸ ì›Œë°ì—…

        ì²« ì˜ˆì¸¡ì€ ëª¨ë¸ ì´ˆê¸°í™”ë¡œ ì¸í•´ ëŠë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
        ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ì „ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
        """
        try:
            self.logger.info("ğŸ”¥ ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")

            # ë”ë¯¸ ì…ë ¥ ìƒì„± (1, 224, 224, 3) â€” ONNX Runtime NHWC í¬ë§·
            dummy_input = np.random.rand(1, 224, 224, 3).astype(np.float32)

            start_time = time.time()
            _ = self._predictor.predict(dummy_input)
            warmup_time = (time.time() - start_time) * 1000

            self.stats['warmup_completed'] = True
            self.logger.info(f"âœ“ ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ ({warmup_time:.0f}ms)")

        except Exception as e:
            self.logger.warning(f"ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")

    def is_ready(self) -> bool:
        """ëª¨ë¸ ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
        return self._predictor is not None and self._predictor.is_ready()

    # â”€â”€â”€ ì˜ˆì¸¡ (ìºì‹œ ì¡°íšŒ â†’ ë¯¸ìŠ¤ì‹œ ì¶”ë¡  â†’ ì €ì¥) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def predict(
        self,
        processed_image,
        use_cache: Optional[bool] = None
    ) -> Tuple[List[Dict[str, any]], bool]:
        """
        ì´ë¯¸ì§€ ì˜ˆì¸¡ (ìºì‹± ì§€ì›)

        Args:
            processed_image: ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ (torch.Tensor ë˜ëŠ” numpy array)
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ ê¸°ë³¸ ì„¤ì • ë”°ë¦„)

        Returns:
            (predictions, from_cache)
              - predictions : ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
              - from_cache   : Trueì´ë©´ ìºì‹œì—ì„œ ë°˜í™˜ëœ ê²°ê³¼
        """
        if not self.is_ready():
            raise PredictionError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        self.stats['total_predictions'] += 1
        should_use_cache = use_cache if use_cache is not None else self.enable_cache

        # â”€â”€ ìºì‹œ ì¡°íšŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if should_use_cache:
            image_hash = self._compute_image_hash(processed_image)
            cached = self._get_from_cache(image_hash)

            if cached is not None:
                self.stats['cache_hits'] += 1
                self.logger.debug(f"âœ“ ìºì‹œ íˆíŠ¸ (í•´ì‹œ: {image_hash[:8]}...)")
                return cached, True          # â† from_cache = True

            self.stats['cache_misses'] += 1

        # â”€â”€ ì‹¤ì œ ì¶”ë¡  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        start_time = time.time()
        predictions = self._predictor.predict(processed_image)
        inference_time = (time.time() - start_time) * 1000
        self.stats['total_inference_time_ms'] += inference_time

        # â”€â”€ ìºì‹œ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if should_use_cache:
            self._save_to_cache(image_hash, predictions)

        return predictions, False            # â† from_cache = False

    # â”€â”€â”€ ìºì‹œ ë‚´ë¶€ êµ¬í˜„ (OrderedDict LRU) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _compute_image_hash(self, image_array) -> str:
        """
        ì´ë¯¸ì§€ ë°°ì—´ì˜ í•´ì‹œê°’ ê³„ì‚° (SHA-256)

        numpy ë°°ì—´ì˜ ë°”ì´íŠ¸ í‘œí˜„ì´ ë™ì¼í•˜ë©´ í•´ì‹œë„ ë™ì¼í•˜ë¯€ë¡œ,
        ë™ì¼í•œ ì´ë¯¸ì§€ì— ëŒ€í•œ ìºì‹œ ì¡°íšŒê°€ ì •í™•íˆ ë™ì‘í•©ë‹ˆë‹¤.
        """
        try:
            if isinstance(image_array, np.ndarray):
                buf = image_array.tobytes()
            else:
                # ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…ì€ ë¬¸ìì—´ í‘œí˜„ìœ¼ë¡œ í•´ì‹œ
                buf = str(image_array).encode('utf-8')
            return hashlib.sha256(buf).hexdigest()
        except Exception:
            # í•´ì‹œ ì‹¤íŒ¨ ì‹œ ëœë¤ ê°’ìœ¼ë¡œ ì¶©ëŒ ìµœì†Œí™”
            return hashlib.sha256(np.random.rand(32).tobytes()).hexdigest()

    def _get_from_cache(self, image_hash: str) -> Optional[List[Dict]]:
        """
        ìºì‹œ ì¡°íšŒ + LRU ìˆœì„œ ê°±ì‹ 

        ì¡°íšŒëœ í‚¤ë¥¼ OrderedDictì˜ endë¡œ ì´ë™í•˜ì—¬
        ìµœê·¼ ì‚¬ìš©ëœ í•­ëª©ì´ ì œê±°ë˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
        """
        if image_hash not in self._cache:
            return None

        # move_to_end â†’ ìµœê·¼ ì‚¬ìš© ì‹œê°„ ê°±ì‹ 
        self._cache.move_to_end(image_hash)
        return self._cache[image_hash]

    def _save_to_cache(self, image_hash: str, predictions: List[Dict]) -> None:
        """
        ìºì‹œ ì €ì¥ + LRU ì •ì±… ì ìš©

        í¬ê¸° ì´ˆê³¼ ì‹œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©(begin)ì„ ì œê±°í•©ë‹ˆë‹¤.
        """
        # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê°’ ê°±ì‹  + endë¡œ ì´ë™
        if image_hash in self._cache:
            self._cache.move_to_end(image_hash)
            self._cache[image_hash] = predictions
            return

        # ìƒˆ í•­ëª© ì¶”ê°€
        self._cache[image_hash] = predictions

        # LRU í‡´ì¥: í¬ê¸° ì´ˆê³¼ ì‹œ begin(oldest) ì œê±°
        while len(self._cache) > self.cache_size:
            self._cache.popitem(last=False)

    # â”€â”€â”€ ëª¨ë¸ ì •ë³´ / í†µê³„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_model_info(self) -> Dict[str, any]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        if self._predictor:
            return self._predictor.get_model_info()
        return {
            'status': 'not_loaded',
            'model_path': self.model_path,
            'labels_path': self.labels_path
        }

    def get_statistics(self) -> Dict[str, any]:
        """
        ì„œë¹„ìŠ¤ í†µê³„ ì¡°íšŒ

        Returns:
            ìºì‹œ íˆíŠ¸ìœ¨, í‰ê·  ì¶”ë¡  ì‹œê°„ ë“± í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        total = self.stats['total_predictions']
        hits = self.stats['cache_hits']
        misses = self.stats['cache_misses']

        cache_hit_rate = (hits / total * 100) if total > 0 else 0.0
        avg_inference_time = (
            self.stats['total_inference_time_ms'] / misses
        ) if misses > 0 else 0.0

        return {
            'total_predictions': total,
            'cache_enabled': self.enable_cache,
            'cache_size': self.cache_size,
            'cache_hits': hits,
            'cache_misses': misses,
            'cache_hit_rate_percent': round(cache_hit_rate, 2),
            'avg_inference_time_ms': round(avg_inference_time, 2),
            'total_inference_time_ms': round(self.stats['total_inference_time_ms'], 2),
            'warmup_completed': self.stats['warmup_completed']
        }

    def clear_cache(self) -> None:
        """ìºì‹œ ì´ˆê¸°í™”"""
        self._cache.clear()
        self.stats['cache_hits'] = 0
        self.stats['cache_misses'] = 0
        self.logger.info("âœ“ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")

    def get_cache_info(self) -> Dict[str, int]:
        """ìºì‹œ ì •ë³´ ì¡°íšŒ"""
        return {
            'hits': self.stats['cache_hits'],
            'misses': self.stats['cache_misses'],
            'maxsize': self.cache_size,
            'currsize': len(self._cache)
        }
