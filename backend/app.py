"""
AI ì§ˆë³‘ ì§„ë‹¨ Flask ì• í”Œë¦¬ì¼€ì´ì…˜ (Production-Ready)

ONNX ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ë£Œ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë³‘ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
Phase 3 Rework: ë°±ì—”ë“œ êµ¬ì¡° ê°œì„ , ëª¨ë¸ ì„œë¹„ìŠ¤ ë ˆì´ì–´ ë¶„ë¦¬, ìºì‹± ë„ì…
"""

import io
from flask import Flask, request
from flask_cors import CORS

from config import get_config
from services import ImageProcessor, ModelService
from utils import (
    # ê²€ì¦
    validate_file,
    # ì‘ë‹µ
    error_response,
    prediction_response,
    # ì˜ˆì™¸
    ModelNotLoadedError,
    ModelLoadError,
    InvalidImageError,
    ImageProcessingError,
    PredictionError,
    FileValidationError,
    # ë¡œê¹…
    setup_logger,
    get_logger,
    log_exception,
    # í—¬ìŠ¤ì²´í¬
    init_health_checker,
    get_health_checker,
    # ê³ ê¸‰ ê²€ì¦
    init_image_validator,
    get_image_validator
)


def create_app(config_name=None):
    """
    Flask ì• í”Œë¦¬ì¼€ì´ì…˜ íŒ©í† ë¦¬ í•¨ìˆ˜ (Production-Ready, Phase 3)
    
    Args:
        config_name (str): í™˜ê²½ ì„¤ì • ì´ë¦„ ('development', 'production', 'testing')
    
    Returns:
        Flask: ì„¤ì •ëœ Flask ì• í”Œë¦¬ì¼€ì´ì…˜
    """
    app = Flask(__name__)
    
    # ===== ì„¤ì • ë¡œë“œ =====
    config = get_config(config_name)
    app.config.from_object(config)
    
    # ===== ë¡œê¹… ì„¤ì • =====
    logger = setup_logger(
        name='aiclassifier',
        log_level=config.LOG_LEVEL,
        log_dir=config.LOG_DIR if hasattr(config, 'LOG_DIR') else None
    )
    
    logger.info("="*70)
    logger.info("ğŸš€ AI ì§ˆë³‘ ì§„ë‹¨ ì„œë²„ ì‹œì‘ (Rework Phase 3)")
    logger.info(f"í™˜ê²½: {config_name or 'default'}")
    logger.info(f"ë””ë²„ê·¸ ëª¨ë“œ: {config.DEBUG}")
    logger.info(f"ëª¨ë¸ ê²½ë¡œ: {config.MODEL_PATH}")
    logger.info("="*70)
    
    # ===== CORS ì„¤ì • (ìƒì„¸) =====
    cors_config = {
        'origins': config.CORS_ORIGINS,
        'methods': config.CORS_METHODS,
        'allow_headers': config.CORS_ALLOW_HEADERS,
        'expose_headers': getattr(config, 'CORS_EXPOSE_HEADERS', []),
        'max_age': getattr(config, 'CORS_MAX_AGE', 3600),
        'supports_credentials': getattr(config, 'CORS_SUPPORTS_CREDENTIALS', False)
    }
    
    CORS(app, **cors_config)
    logger.info(f"âœ“ CORS ì„¤ì • ì™„ë£Œ")
    logger.info(f"  - í—ˆìš©ëœ Origins: {config.CORS_ORIGINS}")
    logger.info(f"  - í—ˆìš©ëœ Methods: {config.CORS_METHODS}")
    
    # ===== í—¬ìŠ¤ì²´ì»¤ ì´ˆê¸°í™” =====
    health_checker = init_health_checker(app)
    
    # ===== ì´ë¯¸ì§€ ê²€ì¦ê¸° ì´ˆê¸°í™” =====
    image_validator = init_image_validator(
        min_width=32,
        min_height=32,
        max_width=4096,
        max_height=4096,
        max_aspect_ratio=10.0
    )
    logger.info("âœ“ ì´ë¯¸ì§€ ê²€ì¦ê¸° ì´ˆê¸°í™”")
    
    # ===== ëª¨ë¸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (Phase 3 - ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ ë ˆì´ì–´) =====
    model_service = ModelService(
        model_path=config.MODEL_PATH,
        labels_path=config.LABELS_PATH,
        enable_cache=getattr(config, 'ENABLE_MODEL_CACHE', True),
        cache_size=getattr(config, 'MODEL_CACHE_SIZE', 128)
    )
    
    try:
        model_service.load_model()
        logger.info("âœ“ ëª¨ë¸ ì„œë¹„ìŠ¤ ë¡œë“œ ì™„ë£Œ")
    except ModelLoadError as e:
        logger.error(f"âœ— ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e.message}")
        logger.warning("âš   ì„œë²„ëŠ” ì‹œì‘ë˜ì§€ë§Œ ì˜ˆì¸¡ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
    
    # ===== ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” =====
    image_processor = ImageProcessor(target_size=config.TARGET_IMAGE_SIZE)
    logger.info("âœ“ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”")
    
    # ===== ë¼ìš°íŠ¸ ì •ì˜ =====
    
    @app.route("/")
    def index():
        """ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸"""
        return {
            "service": "AI Disease Classifier API",
            "version": "7.0.0-phase3",
            "status": "running",
            "environment": config_name or "default",
            "features": {
                "model_caching": model_service.enable_cache,
                "cache_size": model_service.cache_size,
                "warmup": model_service.stats['warmup_completed']
            },
            "endpoints": {
                "health": "/health",
                "health_detailed": "/health/detailed",
                "health_ready": "/health/ready",
                "health_live": "/health/live",
                "model_info": "/model/info",
                "model_stats": "/model/stats",
                "model_cache": "/model/cache",
                "predict": "/predict"
            }
        }
    
    @app.route("/health")
    def health_check():
        """ê°„ë‹¨í•œ í—¬ìŠ¤ì²´í¬"""
        model_status = "ready" if model_service.is_ready() else "not_loaded"
        
        return {
            "status": "healthy" if model_service.is_ready() else "degraded",
            "model": model_status,
            "timestamp": health_checker.get_uptime()['start_time'],
            "version": "7.0.0-phase3"
        }
    
    @app.route("/health/detailed")
    def detailed_health_check():
        """ìƒì„¸ í—¬ìŠ¤ì²´í¬ (ëª¨ë‹ˆí„°ë§ìš©)"""
        return health_checker.comprehensive_health_check(
            predictor=model_service._predictor if model_service._predictor else None,
            cache=None,
            metrics=model_service.get_statistics()
        )
    
    @app.route("/health/ready")
    def readiness_check():
        """Readiness probe (Render/K8sìš©)"""
        import psutil
        
        checks = {
            'model': model_service.is_ready(),
            'disk': psutil.disk_usage('/').percent < 90,
            'memory': psutil.virtual_memory().percent < 90
        }
        
        is_ready = all(checks.values())
        
        return {
            'status': 'ready' if is_ready else 'not_ready',
            'checks': checks
        }, 200 if is_ready else 503
    
    @app.route("/health/live")
    def liveness_check():
        """Liveness probe (Render/K8sìš©)"""
        return {
            'status': 'alive',
            'uptime_seconds': health_checker.get_uptime()['uptime_seconds']
        }, 200
    
    @app.route("/model/info")
    def model_info():
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        return model_service.get_model_info()
    
    @app.route("/model/stats")
    def model_stats():
        """
        ëª¨ë¸ ì„œë¹„ìŠ¤ í†µê³„ ì¡°íšŒ (Phase 3 ì‹ ê·œ ì—”ë“œí¬ì¸íŠ¸)
        
        ìºì‹œ íˆíŠ¸ìœ¨, í‰ê·  ì¶”ë¡  ì‹œê°„ ë“±ì˜ í†µê³„ ì œê³µ
        """
        return {
            "success": True,
            "statistics": model_service.get_statistics(),
            "cache_info": model_service.get_cache_info() if model_service.enable_cache else None
        }
    
    @app.route("/model/cache", methods=['GET', 'DELETE'])
    def model_cache():
        """
        ëª¨ë¸ ìºì‹œ ê´€ë¦¬ (Phase 3 ì‹ ê·œ ì—”ë“œí¬ì¸íŠ¸)
        
        GET: ìºì‹œ ì •ë³´ ì¡°íšŒ
        DELETE: ìºì‹œ ì´ˆê¸°í™”
        """
        if request.method == 'GET':
            if not model_service.enable_cache:
                return {
                    "success": False,
                    "message": "ìºì‹±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤"
                }, 400
            
            return {
                "success": True,
                "cache_info": model_service.get_cache_info(),
                "statistics": {
                    "cache_hits": model_service.stats['cache_hits'],
                    "cache_misses": model_service.stats['cache_misses']
                }
            }
        
        elif request.method == 'DELETE':
            if not model_service.enable_cache:
                return {
                    "success": False,
                    "message": "ìºì‹±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤"
                }, 400
            
            model_service.clear_cache()
            
            return {
                "success": True,
                "message": "ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤"
            }
    
    @app.route("/predict", methods=['POST'])
    def predict():
        """
        ì´ë¯¸ì§€ ì§ˆë³‘ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸ (Production-Grade, Phase 3)
        
        Phase 3 ê°œì„ ì‚¬í•­:
        - ModelServiceë¥¼ í†µí•œ ìºì‹± ì§€ì›
        - ìƒì„¸í•œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì œê³µ
        """
        import time
        start_time = time.time()
        
        request_logger = get_logger('aiclassifier.api')
        request_logger.info("ğŸ“¥ ì˜ˆì¸¡ ìš”ì²­ ìˆ˜ì‹ ")
        
        try:
            # 1. ëª¨ë¸ ì¤€ë¹„ ìƒíƒœ í™•ì¸
            if not model_service.is_ready():
                raise ModelNotLoadedError()
            
            # 2. íŒŒì¼ ì¡´ì¬ í™•ì¸
            if 'file' not in request.files:
                raise FileValidationError("ìš”ì²­ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            
            file = request.files['file']
            
            # 3. ê¸°ë³¸ íŒŒì¼ ê²€ì¦
            is_valid, error_msg = validate_file(
                file,
                allowed_extensions=app.config['ALLOWED_EXTENSIONS'],
                max_size=app.config['MAX_CONTENT_LENGTH']
            )
            
            if not is_valid:
                raise FileValidationError(error_msg)
            
            request_logger.info(f"ğŸ“„ íŒŒì¼ ìˆ˜ì‹ : {file.filename}")
            
            # 4. íŒŒì¼ ì½ê¸°
            in_memory_file = io.BytesIO()
            file.save(in_memory_file)
            in_memory_file.seek(0)
            image_bytes = in_memory_file.read()
            
            # 5. ê³ ê¸‰ ì´ë¯¸ì§€ ê²€ì¦
            if image_validator:
                image_validator.comprehensive_validation(image_bytes)
                request_logger.debug("âœ“ ê³ ê¸‰ ì´ë¯¸ì§€ ê²€ì¦ í†µê³¼")
            
            # 6. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            in_memory_file.seek(0)
            processed_image = image_processor.preprocess(image_bytes)
            
            # 7. ì˜ˆì¸¡ ìˆ˜í–‰ (ìºì‹± ì§€ì›)
            predictions = model_service.predict(processed_image)
            
            # 8. ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time_ms = (time.time() - start_time) * 1000
            
            top_result = predictions[0]
            request_logger.info(
                f"âœ… ì˜ˆì¸¡ ì™„ë£Œ - {file.filename}: "
                f"{top_result['className']} ({top_result['probability']:.4f}) "
                f"[{processing_time_ms:.0f}ms]"
            )
            
            # 9. ì‘ë‹µ ë°˜í™˜ (ë©”íƒ€ë°ì´í„° í¬í•¨)
            response = {
                'success': True,
                'predictions': predictions,
                'metadata': {
                    'processing_time_ms': round(processing_time_ms, 2),
                    'image_size': list(config.TARGET_IMAGE_SIZE),
                    'filename': file.filename,
                    'model_version': '1.0.0-phase3',
                    'cache_enabled': model_service.enable_cache,
                    'from_cache': model_service.stats['cache_hits'] > 0
                }
            }
            
            return response, 200
        
        # ===== ì»¤ìŠ¤í…€ ì˜ˆì™¸ ì²˜ë¦¬ =====
        
        except ModelNotLoadedError as e:
            log_exception(request_logger, e, "ëª¨ë¸ ë¯¸ì¤€ë¹„")
            return error_response(
                e.message,
                status_code=503,
                error_type=e.error_code
            )
        
        except FileValidationError as e:
            request_logger.warning(f"íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {e.message}")
            return error_response(
                e.message,
                status_code=400,
                error_type=e.error_code
            )
        
        except InvalidImageError as e:
            request_logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€: {e.message}")
            return error_response(
                e.message,
                status_code=400,
                error_type=e.error_code
            )
        
        except ImageProcessingError as e:
            log_exception(request_logger, e, "ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜")
            return error_response(
                e.message,
                status_code=422,
                error_type=e.error_code,
                details={
                    "original_error": str(e.original_error)
                } if hasattr(e, 'original_error') and e.original_error else None
            )
        
        except PredictionError as e:
            log_exception(request_logger, e, "ì˜ˆì¸¡ ì˜¤ë¥˜")
            return error_response(
                "ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                status_code=500,
                error_type=e.error_code,
                details={
                    "original_error": str(e.original_error)
                } if hasattr(e, 'original_error') and e.original_error else None
            )
        
        # ===== ì¼ë°˜ ì˜ˆì™¸ ì²˜ë¦¬ =====
        
        except Exception as e:
            log_exception(request_logger, e, "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜")
            return error_response(
                "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                status_code=500,
                error_type="InternalServerError"
            )
    
    # ===== ì „ì—­ ì—ëŸ¬ í•¸ë“¤ëŸ¬ =====
    
    @app.errorhandler(413)
    def request_entity_too_large(error):
        """íŒŒì¼ í¬ê¸° ì´ˆê³¼ ì—ëŸ¬"""
        max_mb = app.config['MAX_CONTENT_LENGTH'] / (1024 * 1024)
        logger.warning(f"íŒŒì¼ í¬ê¸° ì´ˆê³¼: {max_mb}MB ì œí•œ")
        return error_response(
            f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ {max_mb:.0f}MBê¹Œì§€ í—ˆìš©ë©ë‹ˆë‹¤",
            status_code=413,
            error_type="FileTooLargeError"
        )
    
    @app.errorhandler(404)
    def not_found(error):
        """404 ì—ëŸ¬"""
        logger.warning(f"404: {request.path}")
        return error_response(
            "ìš”ì²­í•œ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            status_code=404,
            error_type="NotFoundError"
        )
    
    @app.errorhandler(405)
    def method_not_allowed(error):
        """405 ì—ëŸ¬"""
        logger.warning(f"405: {request.method} {request.path}")
        return error_response(
            f"í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë©”ì†Œë“œì…ë‹ˆë‹¤. {request.method}ëŠ” ì´ ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
            status_code=405,
            error_type="MethodNotAllowedError"
        )
    
    @app.errorhandler(500)
    def internal_server_error(error):
        """500 ì—ëŸ¬"""
        logger.exception("ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜")
        return error_response(
            "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            status_code=500,
            error_type="InternalServerError"
        )
    
    # ===== ë³´ì•ˆ í—¤ë” ì¶”ê°€ =====
    @app.after_request
    def add_security_headers(response):
        """ë³´ì•ˆ í—¤ë” ì¶”ê°€"""
        if getattr(config, 'SECURITY_HEADERS', False):
            response.headers['X-Content-Type-Options'] = 'nosniff'
            response.headers['X-Frame-Options'] = 'DENY'
            response.headers['X-XSS-Protection'] = '1; mode=block'
            
            # HTTPSì—ì„œë§Œ Strict-Transport-Security
            if request.is_secure:
                response.headers['Strict-Transport-Security'] = 'max-age=31536000; includeSubDomains'
        
        return response
    
    logger.info("âœ“ ë¼ìš°íŠ¸ ë° ì—ëŸ¬ í•¸ë“¤ëŸ¬ ë“±ë¡ ì™„ë£Œ")
    logger.info("="*70)
    logger.info("ğŸ‰ ì„œë²„ ì¤€ë¹„ ì™„ë£Œ! Rework Phase 3 ì ìš©ë¨")
    logger.info(f"   - ëª¨ë¸ ìºì‹±: {'í™œì„±í™”' if model_service.enable_cache else 'ë¹„í™œì„±í™”'}")
    logger.info(f"   - ìºì‹œ í¬ê¸°: {model_service.cache_size}")
    logger.info("="*70)
    
    return app


# ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = create_app()


if __name__ == "__main__":
    # ê°œë°œ ì„œë²„ ì‹¤í–‰
    logger = get_logger('aiclassifier')
    logger.info("ğŸ”§ ê°œë°œ ì„œë²„ ì‹œì‘ ì¤‘...")
    
    app.run(
        host='0.0.0.0',
        port=5000,
        debug=app.config['DEBUG']
    )
